{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on  ./camera_cal/calibration10.jpg\n",
      "working on  ./camera_cal/calibration11.jpg\n",
      "working on  ./camera_cal/calibration12.jpg\n",
      "working on  ./camera_cal/calibration13.jpg\n",
      "working on  ./camera_cal/calibration14.jpg\n",
      "working on  ./camera_cal/calibration15.jpg\n",
      "working on  ./camera_cal/calibration16.jpg\n",
      "working on  ./camera_cal/calibration17.jpg\n",
      "working on  ./camera_cal/calibration18.jpg\n",
      "working on  ./camera_cal/calibration19.jpg\n",
      "working on  ./camera_cal/calibration2.jpg\n",
      "working on  ./camera_cal/calibration20.jpg\n",
      "working on  ./camera_cal/calibration3.jpg\n",
      "working on  ./camera_cal/calibration6.jpg\n",
      "working on  ./camera_cal/calibration7.jpg\n",
      "working on  ./camera_cal/calibration8.jpg\n",
      "working on  ./camera_cal/calibration9.jpg\n"
     ]
    }
   ],
   "source": [
    "#Aidos Utegulov\n",
    "#Project 4 Advanced Lane Line Finding\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "#Step 1: Calibrate Camera\n",
    "\n",
    "#Prepare object points, like (0, 0, 0), (1, 0, 0), (2, 0, 0), ... (8, 5, 0)\n",
    "objp = np.zeros((6*9, 3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2) #x, y coordinates\n",
    "\n",
    "#Arrays to store object points and image points from all the images\n",
    "objpoints = [] #3D points in real world space (x, y, z)\n",
    "imgpoints = [] # 2D points in image plane\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('./camera_cal/calibration*.jpg')\n",
    "\n",
    "#Step through the list and search for chessboard corners\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Find the cessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9, 6), None)\n",
    "    \n",
    "    #if found, add object points, image points\n",
    "    if ret == True:\n",
    "        print('working on ', fname)\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "        \n",
    "        #Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (9, 6), corners, ret)\n",
    "        write_name = './output_images/corners_found' + str(idx) + '.jpg'\n",
    "        cv2.imwrite(write_name, img)\n",
    "        \n",
    "#load image for refernece\n",
    "img = cv2.imread('./camera_cal/calibration1.jpg')\n",
    "img_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "#Do camera calibration given object points and image points\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size, None, None)\n",
    "\n",
    "#Save the camera calibration result for later use (we don't worry about rvecs / tvecs)\n",
    "dist_pickle = {}\n",
    "dist_pickle[\"mtx\"] = mtx\n",
    "dist_pickle[\"dist\"] = dist\n",
    "pickle.dump( dist_pickle, open( \"./calibration_pickle.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient and Color thresholding functions needed for further lane line detection steps (code from lessons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def abs_sobel_thresh(img, orient = 'x', sobel_kernel = 3, thresh = (0, 255)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    if orient == 'x':\n",
    "        abs_sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    if orient == 'y':\n",
    "        abs_sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    scaled_sobel = np.uint8(255 * abs_sobel / np.max(abs_sobel))\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def mag_thresh(img, sobel_kernel = 3, mag_thresh = (0, 255)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    gradmag= np.sqrt(sobelx**2 + sobely**2)\n",
    "    scale_factor = np.max(gradmag) / 255\n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8)\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def dir_threshold(img, sobel_kernel = 3, thresh = (0, np.pi/2)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel)\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    binary_output = np.zeros_like(absgraddir)\n",
    "    binary_output[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def color_threshold(image, sthresh = (0, 255), vthresh = (0, 255)):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    v_channel = hsv[:,:,2]\n",
    "    v_binary = np.zeros_like(v_channel)\n",
    "    v_binary[(v_channel >= vthresh[0]) & (v_channel <= vthresh[1])] = 1\n",
    "    \n",
    "    yellow = cv2.inRange(hsv, (20, 10, 0, 100), (50, 255, 255))\n",
    "    sensitivity1 = 68\n",
    "    white = cv2.inRange(hsv, (0, 0, 255 - sensitivity1), (255, 20, 255))\n",
    "    \n",
    "    sensitivity2 = 60\n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "    white_2 = cv2.inRange(hls, (0, 255 - sensitivity2, 0), (255, 255, sensitivity2))\n",
    "    white_3 = cv2.inRange(hls, (200, 200, 200), (255, 255, 255))\n",
    "    \n",
    "    s_channel = hls[:,:,2]\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= sthresh[0]) & (s_channel <= sthresh[1])] = 1\n",
    "    \n",
    "    output = np.zeros_like(s_channel)\n",
    "    output[((s_binary == 1) & (v_binary == 1)) | (yellow == 1) | (white == 1) | (white_2 == 1) | (white_3 == 1)] = 1\n",
    "    return output\n",
    "\n",
    "def window_mask(width, height, img_ref, center, level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level + 1) * height):int(img_ref.shape[0]-level*height), \n",
    "            max(0, int(center-width)):min(int(center + width), img_ref.shape[1])] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Performing image undistortion in the test_images directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global polygon_points_old\n",
    "polygon_points_old = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in the saved objpoints and imgpoints\n",
    "dist_pickle = pickle.load( open( \"./calibration_pickle.p\", \"rb\" ) )\n",
    "mtx = dist_pickle[\"mtx\"]\n",
    "dist = dist_pickle[\"dist\"]\n",
    "from tracker import tracker\n",
    "\n",
    "\n",
    "def undistort_img(img):\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist\n",
    "\n",
    "def threshold_img(img):   \n",
    "    #process image and generate binary pixel of interests\n",
    "    preprocessImage = np.zeros_like(img[:,:,0])\n",
    "    gradx = abs_sobel_thresh(img, orient='x', thresh = (12, 255)) #12\n",
    "    grady = abs_sobel_thresh(img, orient='y', thresh = (25, 255)) #25\n",
    "    grad_mag = mag_thresh(img, mag_thresh = (30, 120))\n",
    "    grad_dir = dir_threshold(img, thresh = (0.7, 1.3))\n",
    "    c_binary = color_threshold(img, sthresh = (100, 255), vthresh = (50, 255))\n",
    "    preprocessImage[((gradx == 1) & (grady == 1) | (c_binary == 1))] = 255\n",
    "    \n",
    "    return preprocessImage\n",
    "\n",
    "def persp_transform(img):\n",
    "    \n",
    "    #work on defining perspective transformation area\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    bot_width = .76 #percent of bottom trapezoid height\n",
    "    mid_width = .08 #percent of middle trapezoid height\n",
    "    height_pct = .63 #percent for trapezoid height\n",
    "    bottom_trim = .933 #percent from top to bottom to avoid car hood\n",
    "    src = np.float32([[img.shape[1] * (.5-mid_width/2), img.shape[0]*height_pct],[img.shape[1] * (.5+mid_width/2),\n",
    "                                                                                 img.shape[0] * height_pct], \n",
    "                      [img.shape[1] * (.5 + bot_width / 2), img.shape[0] * bottom_trim], \n",
    "                      [img.shape[1] * (0.5 - bot_width / 2), img.shape[0] * bottom_trim]])\n",
    "    offset = img_size[0] * .25\n",
    "    dst = np.float32([[offset, 0], [img_size[0] - offset, 0],[img_size[0] - offset, img_size[1]], [offset, img_size[1]]])\n",
    "    \n",
    "    #perform the transform\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags = cv2.INTER_LINEAR)\n",
    "\n",
    "    return warped, Minv\n",
    "\n",
    "\n",
    "    \n",
    "def find_lanes(warped, calibrated_img, Minv):\n",
    "    window_width = 25\n",
    "    window_height = 80\n",
    "    img_size = (calibrated_img.shape[1], calibrated_img.shape[0])\n",
    "    \n",
    "    # Set up the overall class to do all the tracking\n",
    "    curve_centers = tracker(Mywindow_width = window_width, Mywindow_height = window_height, \n",
    "                            Mymargin = 25, My_ym = 30/720, My_xm = 3.7/700, Mysmooth_factor = 15)\n",
    "    \n",
    "    window_centroids = curve_centers.find_window_centroids(warped)\n",
    "    \n",
    "    #Points used to draw all the left and right windows\n",
    "    l_points = np.zeros_like(warped)\n",
    "    r_points = np.zeros_like(warped)\n",
    "    \n",
    "    # Points used to find the left and right lanes\n",
    "    rightx = []\n",
    "    leftx = []\n",
    "    \n",
    "    #Go through each level and draw the windows\n",
    "    for level in range(0, len(window_centroids)):\n",
    "        # Window_mask is a function to draw window areas\n",
    "        # add center value found in frame to the list of lane points per left, right\n",
    "        leftx.append(window_centroids[level][0])\n",
    "        rightx.append(window_centroids[level][1])\n",
    "        l_mask = window_mask(window_width, window_height, warped, window_centroids[level][0], level)\n",
    "        r_mask = window_mask(window_width, window_height, warped, window_centroids[level][1], level)\n",
    "        # Add graphic points from window mask here to total pixels found\n",
    "        l_points[(l_points == 255) | ((l_mask == 1) )] = 255\n",
    "        r_points[(r_points == 255) | ((r_mask == 1) )] = 255\n",
    "        \n",
    "    # Draw the results\n",
    "    template = np.array(r_points + l_points, np.uint8) # add both left and right window pixels together\n",
    "    zero_channel = np.zeros_like(template) # create a zero color channel\n",
    "    template = np.array(cv2.merge((zero_channel, template, zero_channel)), np.uint8) # make window pixels green\n",
    "    warpage = np.array(cv2.merge((warped, warped, warped)), np.uint8) # making the original road pixels 3 color channels\n",
    "    result = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the original road image with window results\n",
    "    \n",
    "    cv2.imwrite('./output_images/overlay.jpg', result)\n",
    "    \n",
    "    # fit the lane boundaries to the left, right center positions found\n",
    "    yvals = range(0, warped.shape[0])\n",
    "    \n",
    "    res_yvals = np.arange(warped.shape[0] - (window_height/2), 0, -window_height)\n",
    "    \n",
    "    left_fit = np.polyfit(res_yvals, leftx, 2)\n",
    "    left_fitx = left_fit[0] * yvals * yvals + left_fit[1] * yvals + left_fit[2]\n",
    "    left_fitx = np.array(left_fitx, np.int32)\n",
    "    \n",
    "    right_fit = np.polyfit(res_yvals, rightx, 2)\n",
    "    right_fitx = right_fit[0] * yvals * yvals + right_fit[1] * yvals + right_fit[2]\n",
    "    right_fitx = np.array(right_fitx, np.int32)\n",
    "    \n",
    "    left_lane = np.array(list(zip(np.concatenate((left_fitx - window_width / 2, left_fitx[::-1] + window_width / 2),\n",
    "                                                axis = 0), np.concatenate((yvals, yvals[::-1]), axis = 0))), np.int32)\n",
    "    \n",
    "    right_lane = np.array(list(zip(np.concatenate((right_fitx - window_width / 2, right_fitx[::-1] + window_width / 2),\n",
    "                                                 axis = 0), np.concatenate((yvals, yvals[::-1]), axis = 0))), np.int32)\n",
    "    middle_marker = np.array(list(zip(np.concatenate((left_fitx + window_width / 2, right_fitx[::-1] - window_width / 2),\n",
    "                                                    axis = 0), np.concatenate((yvals, yvals[::-1]), axis = 0))), np.int32)\n",
    "\n",
    "    road = np.zeros_like(calibrated_img)\n",
    "    road_bkg = np.zeros_like(calibrated_img)\n",
    "    cv2.fillPoly(road, [left_lane], color = [255, 0, 0])\n",
    "    cv2.fillPoly(road, [right_lane], color = [0, 0, 255])\n",
    "    cv2.fillPoly(road, [middle_marker], color = [0, 255, 0])\n",
    "    cv2.fillPoly(road_bkg, [left_lane], color = [255, 255, 255])\n",
    "    cv2.fillPoly(road_bkg, [right_lane], color = [255, 255, 255])\n",
    "    \n",
    "    road_warped = cv2.warpPerspective(road, Minv, img_size, flags = cv2.INTER_LINEAR)\n",
    "    road_warped_bkg = cv2.warpPerspective(road_bkg, Minv, img_size, flags = cv2.INTER_LINEAR)\n",
    "    \n",
    "    base = cv2.addWeighted(calibrated_img, 1.0, road_warped_bkg, -1.0, 0.0)\n",
    "    result = cv2.addWeighted(base, 1.0, road_warped, 0.7, 0.0)\n",
    "    \n",
    "    ym_per_pix = curve_centers.ym_per_pix # meters per pixel in y dimension\n",
    "    xm_per_pix = curve_centers.xm_per_pix # meters per pixel in x dimension\n",
    "    \n",
    "    curve_fit_cr = np.polyfit(np.array(res_yvals, np.float32) * ym_per_pix, np.array(leftx, np.float32) * xm_per_pix, 2)\n",
    "    curverad = ((1 + (2 * curve_fit_cr[0] * yvals[-1] * ym_per_pix + curve_fit_cr[1])**2)**1.5)/np.absolute(2 * curve_fit_cr[0])\n",
    "    \n",
    "    \n",
    "    # calculate the offset of the car on the road\n",
    "    camera_center = (left_fitx[-1] + right_fitx[-1]) / 2\n",
    "    center_diff = (camera_center - warped.shape[1] / 2) * xm_per_pix\n",
    "    side_pos = 'left'\n",
    "    if center_diff <= 0:\n",
    "        side_pos = 'right'\n",
    "        \n",
    "    # draw the text showing curvature, offset, and speed\n",
    "    cv2.putText(result, 'Radius of Curvature = ' + str(round(curverad,3)) + '(m)', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                1, (255, 255, 255), 2)\n",
    "    cv2.putText(result,'Vehicle is ' + str(abs(round(center_diff, 3))) + 'm ' + side_pos + ' of center', (50, 100),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    return result\n",
    "\n",
    "def my_pipeline(img):\n",
    "    undistorted_img = undistort_img(img)\n",
    "    thresholded_img = threshold_img(undistorted_img)\n",
    "    warped_img, Minv = persp_transform(thresholded_img)\n",
    "    result = find_lanes(warped_img, undistorted_img, Minv)\n",
    "    return result\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Images for the modules in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = glob.glob('./test_images/test*.jpg')\n",
    "\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    result = my_pipeline(img)\n",
    "    write_name = './output_images/tracked' + str(idx) + '.jpg'\n",
    "    cv2.imwrite(write_name, result)\n",
    "\n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    result = undistort_img(img)\n",
    "    write_name = './output_images/undistorted' + str(idx) + '.jpg'\n",
    "    cv2.imwrite(write_name, result)\n",
    "    \n",
    "images = glob.glob('./output_images/undistorted*.jpg')\n",
    "    \n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    result = threshold_img(img)\n",
    "    write_name = './output_images/thresholded' + str(idx) + '.jpg'\n",
    "    cv2.imwrite(write_name, result)\n",
    "\n",
    "images = glob.glob('./output_images/thresholded*.jpg')    \n",
    "for idx, fname in enumerate(images):\n",
    "    img = cv2.imread(fname)\n",
    "    result, Minv = persp_transform(img)\n",
    "    write_name = './output_images/warped' + str(idx) + '.jpg'\n",
    "    cv2.imwrite(write_name, result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Finding lane markers in the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_tracked.mp4\n",
      "[MoviePy] Writing video output_tracked.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [06:57<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_tracked.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "Output_video = 'output_tracked.mp4'\n",
    "Input_video = 'project_video.mp4'\n",
    "\n",
    "clip1 = VideoFileClip(Input_video)\n",
    "video_clip = clip1.fl_image(my_pipeline) # NOTE: this function expects color images!!\n",
    "video_clip.write_videofile(Output_video, audio = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
